# K-Nearest-Neighbours-Classifier

K Nearest Neighbors (KNN) is one of the simplest machine learning algorithms. In this algorithm, a sample is categorized by a majority vote of its neighbors and is determined in the most general class among k close neighbors. K is a positive value and is generally small. If k = 1, the sample is easily determined in its nearest neighbor class. For k, being an odd number is useful because it prevents equal votes.

The KNN method is applicable to many problems because it is effective, non-parametric, and easy to implement. However, its classification time is long, and it isn't easy to find the optimal k value. The best choice of k depends on the data. In general, a large amount of k reduces the effect of noise on the classification, but the boundary between classes becomes less distinct.

<br/><br/>

<p align="center">
  <img width="350" height="310" src="https://user-images.githubusercontent.com/66460485/128825219-0095b65c-a17e-4960-b327-604c8bf737e5.png">
</p>


#Class Attribute
>Types of glass

<ol>
<li>building_windows_float_processed</li>
<li>building_windows_non_float_processed</li>
<li>vehicle_windows_float_processed</li>
<li>vehicle_windows_non_float_processed</li>
<li>containers</li>
<li>tableware</li>
<li>headlamps</li>
<li>vehicle_windows_float_processed</li>
</ol>
